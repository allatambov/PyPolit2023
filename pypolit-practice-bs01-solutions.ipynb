{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы программирования в Python \n",
    "\n",
    "*Алла Тамбовцева*\n",
    "\n",
    "## Практикум 13. Парсинг с `BeautifulSoup`: тэги и атрибуты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже немного познакомились со структурой html-страниц и поиском информации по тэгам, теперь попробуем выгрузить информацию из более интересной страницы, а точнее, с сайта [nplus1.ru](https://nplus1.ru/). Наша задача – выгрузить недавние новости в датафрейм `pandas`, чтобы потом сохранить все в файл Excel.\n",
    "\n",
    "Для работы нам снова понадобится модуль `requests` для отправки запросов, для «подключения» к странице и получения ее содержимого в виде строки, и функция `BeautifulSoup` из библиотеки `bs4` для удобного поиска по полученной строке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним ссылку на главную страницу в переменную `main` и отправим запрос к ней с помощью функции `get()` из `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = \"https://nplus1.ru/\"\n",
    "page = requests.get(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заберём исходный код страницы и преобразуем строку с ним в объект `BeautifulSoup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сгрузить все новости с главной страницы сайта, нужно собрать все ссылки на страницы с этими новостями. Ссылки в html-файле всегда заключены в тэг `<a></a>` и имеют атрибут `href`. Найдем кусочки кода HTML, соответствующие всем ссылкам на главной странице сайта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/768\">Генетика</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/890\">Математика</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/871\">Космонавтика</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/876\">Археология</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/775\">Нейронауки</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/767\">На мышах</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/771\">Звук</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/772\">Красота</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/778\">Научные закрытия</a>,\n",
       " <a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/917\">ИИ спешит на помощь</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_raw = soup.find_all(\"a\") \n",
    "links_raw[10:20]  # несколько штук для примера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент возвращаемого списка имеет тип `BeautifulSoup` и структуру, очень похожую на словарь. Например, ссылка `<a class=\"hover:underline transition-colors duration-75\" href=\"/search/empty/768\">Генетика</a>` изнутри выглядит как словарь следующего вида:\n",
    "\n",
    "    {'href' : '/search/empty/768', \n",
    "     'class' : 'hover:underline transition-colors duration-75'}.\n",
    "    \n",
    "Как мы помним, значение по ключу из словаря можно вызвать с помощью метода `.get()`. Давайте извлечем значения по ключу `href` из каждого элемента списка `links`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/search/empty/768',\n",
       " '/search/empty/890',\n",
       " '/search/empty/871',\n",
       " '/search/empty/876',\n",
       " '/search/empty/775',\n",
       " '/search/empty/767',\n",
       " '/search/empty/771',\n",
       " '/search/empty/772',\n",
       " '/search/empty/778',\n",
       " '/search/empty/917']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [li.get(\"href\") for li in links_raw] \n",
    "links[10:20]  # несколько штук для примера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылок в списке выше много. Но нам нужны только новости – ссылки, которые начинаются с `https://nplus1.ru/news`. Создадим пустой список `news` и будем добавлять в него только ссылки, которые удовлетворяют этому условию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for li in links:\n",
    "    if \"https://nplus1.ru/news/\" in li:\n",
    "        news.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://nplus1.ru/news/2015/09/21/editor-thy-name',\n",
       " 'https://nplus1.ru/news/2023/05/19/octopus-nightmare',\n",
       " 'https://nplus1.ru/news/2023/05/20/blue-origin-win-artemis',\n",
       " 'https://nplus1.ru/news/2023/05/20/inscultura',\n",
       " 'https://nplus1.ru/news/2023/05/20/neanderthal-bone-industry',\n",
       " 'https://nplus1.ru/news/2023/05/19/permafrost',\n",
       " 'https://nplus1.ru/news/2023/05/19/ancient-hearth',\n",
       " 'https://nplus1.ru/news/2023/05/19/x-ray-forgery-update',\n",
       " 'https://nplus1.ru/news/2023/05/19/hoolock-tianxing',\n",
       " 'https://nplus1.ru/news/2023/05/19/sn-2020-eyj']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая ссылка ведет не совсем на новость, скорее, на объявление, поэтому давайте ее уберем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наша задача сводится к следующему: изучить одну страницу с новостью, научиться из нее вытаскивать текст и всю необходимую информацию, а потом применить весь набор действий к каждой ссылке из `news` в цикле. Посмотрим на новость с индексом 0, у вас может быть другая, новости обновляются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://nplus1.ru/news/2023/05/19/octopus-nightmare\n"
     ]
    }
   ],
   "source": [
    "link0 = news[0]\n",
    "print(link0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1\n",
    "\n",
    "Отправьте запрос к странице по ссылке `link0` с одной новостью, получите результат в виде объекта `BeautifulSoup` и сохраните его как `soup0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page0 = requests.get(link0)\n",
    "soup0 = BeautifulSoup(page0.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2\n",
    "\n",
    "Найдите заголовок новости и сохраните его в переменную `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup0.find(\"title\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3\n",
    "\n",
    "Найдите имя автора новости и дату её публикации. Сохраните их в `author` и `date` соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 Сергей Коленов\n"
     ]
    }
   ],
   "source": [
    "date = soup0.find(\"meta\", {\"itemprop\" : \"datePublished\"}).get(\"content\") \n",
    "author = soup0.find(\"meta\", {\"name\" : \"author\"}).get(\"content\") \n",
    "print(date, author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "\n",
    "Найдите сложность новости и рубрики, к которым она относится. Сохраните сложность в переменную `diffc`. Рубрики сначала можно сохранить в список, а затем его элементы объединить в одну строку `rubs`.\n",
    "\n",
    "**Подсказка:** чтобы упростить себе жизнь, найдите сначала раздел (`div`), в котором на странице хранится дата и время публикации, сложность и рубрики, а затем выполняйте поиск в рамках этого раздела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зоология\n"
     ]
    }
   ],
   "source": [
    "# начнем в следующий раз отсюда\n",
    "\n",
    "div = soup0.find(\"div\", \n",
    "                 {\"class\" : \"flex flex-wrap lg:mb-10 gap-2 text-tags xl:pr-9\"})\n",
    "diffc = div.find_all(\"span\")[3].text\n",
    "rubs_raw = div.find_all(\"span\")[4:] \n",
    "rubs_L = [r.text for r in rubs_raw] \n",
    "rubs = \", \".join(rubs_L)\n",
    "print(rubs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5\n",
    "\n",
    "Соберите из абзацев текст новости и сохраните его в переменную `text`. Избавьтесь от постронних символов (`\\xa0`, `\\n`) в тексте. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6\n",
    "\n",
    "Напишите функцию `get_news()`, которая принимает на вход ссылку на страницу с одной новостью, а возвращает список из следующих характеристик: имя автора, дата публикации, сложность новости, рубрики, текст новости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь осталось применить функцию ко всем ссылкам в списке `news`. Чтобы сайт не понял, что мы его автоматически грабим, будем выгружать новости постепенно – с задержкой в 1.5 секунды. Импортируем для этого функцию `sleep` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем применять функцию в цикле к каждой ссылке в `news`, только с одним дополнением – добавленной конструкцией `try-except`, которая позволит продолжать исполнение цикла, если при применении функции Python столкнулся с ошибкой любого вида:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for n in news:\n",
    "    # пробуй исполнить следующий код\n",
    "    try:\n",
    "        res = get_news(n)\n",
    "        info.append(res)\n",
    "        print(n)\n",
    "    # если он вызвал ошибку, печатай сообщение и иди дальше\n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "        print(n)\n",
    "    sleep(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько элементов `info`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальный штрих – импортируем `pandas` и преобразуемый полученный список кортежей в датафрейм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(info)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим содержательные названия столбцов и выгрузим датафрейм в файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"title\", \"author\", \"date\", \"diffc\", \"rubrics\", \"text\"]\n",
    "df.to_excel(\"nplus1.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
